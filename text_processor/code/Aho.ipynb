{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.text_rank import TextRankSummarizer as Summarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_form_file(text_file, language=\"english\", sentences_cout=10):\n",
    "\n",
    "    parser = PlaintextParser.from_file(text_file, Tokenizer(language))\n",
    "    stemmer = Stemmer(language)\n",
    "\n",
    "    summarizer = Summarizer(stemmer)\n",
    "    summarizer.stem_words = get_stop_words(language)\n",
    "\n",
    "    sentences = summarizer(parser.document, sentences_cout)\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def sum_form_url(url, language=\"english\", sentences_cout=10):\n",
    "\n",
    "    parser = HtmlParser.from_url(url, Tokenizer(language))\n",
    "    stemmer = Stemmer(language)\n",
    "\n",
    "    summarizer = Summarizer(stemmer)\n",
    "    summarizer.stem_words = get_stop_words(language)\n",
    "\n",
    "    sentences = summarizer(parser.document, sentences_cout)\n",
    "\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sum_form_url('https://www.nature.com/articles/s41467-017-02681-z', sentences_cout=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'Using the WSBM to uncover a network\\u2019s community structure involves inferring both the parameters of these processes and the nodes\\u2019 community assignments that maximize the log-evidence that the WSBM generated the observed network (see Methods for details on connectome reconstruction and the WSBM).'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[0]._text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"relation_extraction/\")\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "\n",
    "corenlp = StanfordCoreNLP('http://localhost:9000')\n",
    "corenlp_properties = {\n",
    "    'annotators': 'tokenize, pos, ner',\n",
    "    'outputFormat': 'json'\n",
    "}\n",
    "\n",
    "def get_tagged_from_server(input_text):\n",
    "    \"\"\"\n",
    "    Send the input_text to the CoreNLP server and retrieve the tokens, named entity tags and part-of-speech tags.\n",
    "    \"\"\"\n",
    "    corenlp_output = corenlp.annotate(input_text,properties=corenlp_properties).get(\"sentences\", [])[0]\n",
    "    tagged = [(t['originalText'], t['ner'], t['pos']) for t in corenlp_output['tokens']]\n",
    "    return tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'Using', u'O', u'VBG'),\n (u'the', u'O', u'DT'),\n (u'WSBM', u'O', u'NNP'),\n (u'to', u'O', u'TO'),\n (u'uncover', u'O', u'VB'),\n (u'a', u'O', u'DT'),\n (u'networks', u'O', u'NNS'),\n (u'community', u'O', u'NN'),\n (u'structure', u'O', u'NN'),\n (u'involves', u'O', u'VBZ'),\n (u'inferring', u'O', u'VBG'),\n (u'both', u'O', u'CC'),\n (u'the', u'O', u'DT'),\n (u'parameters', u'O', u'NNS'),\n (u'of', u'O', u'IN'),\n (u'these', u'O', u'DT'),\n (u'processes', u'O', u'NNS'),\n (u'and', u'O', u'CC'),\n (u'the', u'O', u'DT'),\n (u'nodes', u'O', u'NNS'),\n (u'community', u'O', u'NN'),\n (u'assignments', u'O', u'NNS'),\n (u'that', u'O', u'WDT'),\n (u'maximize', u'O', u'VBP'),\n (u'the', u'O', u'DT'),\n (u'log-evidence', u'O', u'NN'),\n (u'that', u'O', u'IN'),\n (u'the', u'O', u'DT'),\n (u'WSBM', u'ORGANIZATION', u'NNP'),\n (u'generated', u'O', u'VBD'),\n (u'the', u'O', u'DT'),\n (u'observed', u'O', u'VBN'),\n (u'network', u'O', u'NN'),\n (u'(', u'O', u'-LRB-'),\n (u'see', u'O', u'VB'),\n (u'Methods', u'O', u'NNS'),\n (u'for', u'O', u'IN'),\n (u'details', u'O', u'NNS'),\n (u'on', u'O', u'IN'),\n (u'connectome', u'O', u'NN'),\n (u'reconstruction', u'O', u'NN'),\n (u'and', u'O', u'CC'),\n (u'the', u'O', u'DT'),\n (u'WSBM', u'O', u'NNP'),\n (u')', u'O', u'-RRB-'),\n (u'.', u'O', u'.')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tagged_from_server(sentences[0]._text.encode('ascii','ignore'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sentences[0]._text.encode('ascii','ignore'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
